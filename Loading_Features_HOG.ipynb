{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.utils import Bunch\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage import io\n",
    "from skimage.filters import threshold_otsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  2019-01-16 11:03:35.336890\n"
     ]
    }
   ],
   "source": [
    "# Start point\n",
    "start_time = time.time()\n",
    "print(\"Start time: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HOG(image):\n",
    "    img = hog(image,\n",
    "              orientations=8,\n",
    "              pixels_per_cell=(4, 4),\n",
    "              cells_per_block=(2, 2),\n",
    "              block_norm='L2-Hys',\n",
    "              multichannel = False,\n",
    "              feature_vector = True)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarization using Otsu's method\n",
    "def binarize(inp_image):\n",
    "    thresh = threshold_otsu(inp_image)\n",
    "    binary_thresh_img = inp_image > thresh\n",
    "\n",
    "    img_binary = img_as_ubyte(binary_thresh_img)\n",
    "\n",
    "    return img_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images in structured directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n fixed genuine signature + 1 genuine or forgery signature\n",
    "\n",
    "def match_images(flat_data, target, n):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    \n",
    "    - flat_data: a list of ndarray containing the flattern information coming from \n",
    "      the hog on the signature image.\n",
    "    - target: a list of strings. Each component can be 'Genuine' of 'Forgery'.\n",
    "    - n: number of fixed genuine signtures in each match. \n",
    "   \n",
    "    Returns\n",
    "    \n",
    "    A dataset divided in:\n",
    "    - data: a list of 20-n ndarray, each one created by concatenation of n+1 array of the input flat_data list.\n",
    "    - target: a list of 20-n strings, each one created by concatenation of n+1 strings of input target list.\n",
    "    \"\"\"\n",
    "      \n",
    "    m_flat_data=[]\n",
    "    m_target=[]\n",
    "    g_indexes=[ind for ind, t in enumerate(target) if t=='Genuine']\n",
    "    f_indexes=[ind for ind, t in enumerate(target) if t=='Forgery']\n",
    "    \n",
    "    fixed_g = flat_data[g_indexes[0]]\n",
    "    for i in range(1,n):\n",
    "        fixed_g = np.concatenate((fixed_g, flat_data[g_indexes[i]]))\n",
    "    \n",
    "    for i in range(n,10):\n",
    "        m_flat_data.append(np.concatenate((fixed_g,flat_data[g_indexes[i]])))\n",
    "        m_target.append(+1) # +1 is the positive class\n",
    "        \n",
    "    for i in range(0,10):\n",
    "        m_flat_data.append(np.concatenate((fixed_g,flat_data[f_indexes[i]])))\n",
    "        m_target.append(-1) # -1 is the negative class\n",
    "    \n",
    "    return Bunch(data=m_flat_data,\n",
    "                 target=m_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_files_split(container_path, dimension=(100, 144)): # height x lenght\n",
    "    \"\"\"\n",
    "    Load image files with categories as subfolder names which performs like scikit-learn sample dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    container_path : string or unicode\n",
    "        Path to the main folder holding one subfolder per category\n",
    "    dimension : tuple\n",
    "        size to which image are adjusted to\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dataset splitted in Training Set and Test Set as follows:\n",
    "    - 70% of subjects in the Training Set\n",
    "    - 30% of subjects in the Test Set \n",
    "    \"\"\"\n",
    "    \n",
    "    image_dir = Path(container_path)\n",
    "    subj = [directory for directory in image_dir.iterdir() if directory.is_dir()]\n",
    "    categories = ['Genuine', 'Forgery']\n",
    "\n",
    "    flat_data_train = []\n",
    "    target_train = []\n",
    "    flat_data_test = []\n",
    "    target_test = []\n",
    "    \n",
    "    stop_load_train = 110\n",
    "\n",
    "    for n, s in enumerate(subj):        \n",
    "        folders = [sub_directory for sub_directory in s.iterdir() if sub_directory.is_dir()]\n",
    "        temp_flat_data=[] # Temporary list of flat_data used to store the information of the current subject \n",
    "                          # and reinitialized as empty list for each subject\n",
    "        temp_target=[]    # Temporary list of target\n",
    "        \n",
    "        for sub_dir in folders:\n",
    "            if sub_dir.name in categories: # Skip the Disguised folder\n",
    "                for file in sub_dir.iterdir():\n",
    "                    img_gray = img_as_ubyte(io.imread(file, as_gray=True)) # load the images in grayscale\n",
    "                    img_bin = binarize(img_gray)\n",
    "                    img_resized = resize(img_bin, dimension, anti_aliasing=True, mode='reflect')\n",
    "                    temp_flat_data.append(HOG(img_resized))\n",
    "                    temp_target.append(sub_dir.name)\n",
    "\n",
    "        subj_dataset = match_images(temp_flat_data, temp_target, 2) # 2 fixed signatures for each person\n",
    "\n",
    "        if n < stop_load_train:   \n",
    "            flat_data_train += subj_dataset.data\n",
    "            target_train += subj_dataset.target\n",
    "        else:\n",
    "            flat_data_test += subj_dataset.data\n",
    "            target_test += subj_dataset.target\n",
    "            \n",
    "            \n",
    "    flat_data_train = np.array(flat_data_train)\n",
    "    target_train = np.array(target_train)\n",
    "    flat_data_test = np.array(flat_data_test)\n",
    "    target_test = np.array(target_test)\n",
    "    \n",
    "    print(target_train.dtype)\n",
    "    print(target_train)\n",
    "\n",
    "    return Bunch(X_train= flat_data_train,\n",
    "                 X_test = flat_data_test,\n",
    "                 y_train= target_train,\n",
    "                 y_test = target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "[ 1  1  1 ... -1 -1 -1]\n",
      "----Dataset Loaded----\n"
     ]
    }
   ],
   "source": [
    "# To let the code run it is necessary to put this script in the same folder which contains the dataset folder\n",
    "\n",
    "db_folder='SignUniPD_anonymised' # Modify with the name of the folder containing the dataset.\n",
    "image_dataset = load_image_files_split(os.path.join(os.getcwd(),db_folder))\n",
    "\n",
    "print(\"----Dataset Loaded----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creation of the pickle which is goig to store the loaded Dataset.\n",
    "\n",
    "pickle_out = open(\"Signature_Dataset_HOG_110_3_int.pickle\",\"wb\") #Modify with the name of the dataset you're saving\n",
    "pickle.dump(image_dataset, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "print(\"----Pickle created----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#End point \n",
    "\n",
    "end_time = time.time()\n",
    "uptime = end_time - start_time\n",
    "human_uptime = datetime.timedelta(seconds=uptime)\n",
    "\n",
    "print(\"Start time: \", datetime.datetime.now())\n",
    "print(\"End time: \", datetime.datetime.now())\n",
    "print(\"Uptime :\" ,human_uptime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

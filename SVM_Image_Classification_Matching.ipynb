{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification using `sklearn.svm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import svm, metrics, datasets\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# My addition to the original code\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "from random import randint\n",
    "\n",
    "import os\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage import io\n",
    "from skimage.filters import threshold_otsu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  2019-01-03 17:22:52.193112\n"
     ]
    }
   ],
   "source": [
    "# Start point\n",
    "start_time = time.time()\n",
    "print(\"Start time: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HOG(image):\n",
    "    img = hog(image,\n",
    "              orientations=8,\n",
    "              pixels_per_cell=(4, 4),\n",
    "              cells_per_block=(2, 2),\n",
    "              block_norm='L2-Hys',\n",
    "              multichannel = False,\n",
    "              feature_vector = True)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarization using Otsu's method\n",
    "def binarize(inp_image):\n",
    "    thresh = threshold_otsu(inp_image)\n",
    "    binary_thresh_img = inp_image > thresh\n",
    "\n",
    "    img_binary = img_as_ubyte(binary_thresh_img)\n",
    "\n",
    "    return img_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining things for the morphological filtering\n",
    "\n",
    "from skimage.morphology import erosion, dilation, opening, closing, white_tophat\n",
    "from skimage.morphology import black_tophat, skeletonize, convex_hull_image\n",
    "from skimage.morphology import disk, square, diamond\n",
    "\n",
    "selem = disk(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images in structured directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that builds pair of signatures on which the algorithm is going to work.\n",
    "\n",
    "def match_2(flat_data, target, xn):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "\n",
    "    - flat_data: a list of ndarray containing the flattern information coming from\n",
    "      the hog on the signature image.\n",
    "    - target: a list of strings. Each component can be 'Genuine' of 'Forgery'.\n",
    "\n",
    "    Returns\n",
    "\n",
    "    A dataset divided in:\n",
    "    - data: a list of ndarray, each one originating from the concatenation of 2 array of the input flat_data.\n",
    "    - target: a list of strings, each one deriving from the concatenation of 2 strings of input target list.\n",
    "\n",
    "    -------------------------------------------------------\n",
    "\n",
    "    Rule of matching:\n",
    "    - The first xn couples are composed of 2 genuine signatures randomly choosen between the 10 available.\n",
    "    - The latter xn couples are composed of 2 forgery signatures randomly choosen.\n",
    "    \"\"\"\n",
    "\n",
    "    m_flat_data=[]\n",
    "    m_target=[]\n",
    "    g_indexes=[ind for ind, t in enumerate(target) if t=='Genuine']\n",
    "    f_indexes=[ind for ind, t in enumerate(target) if t=='Forgery']\n",
    "    matches=[]\n",
    "    for i in range(0,xn): # Creates xn matchings: Genuine - Genuine\n",
    "        n=randint(0,len(g_indexes)-1)\n",
    "        m=randint(0,len(g_indexes)-1)\n",
    "        while n == m or (g_indexes[n],g_indexes[m]) in matches:\n",
    "            n=randint(0,len(g_indexes)-1)\n",
    "            m=randint(0,len(g_indexes)-1)\n",
    "        m_flat_data.append(np.concatenate((flat_data[g_indexes[n]],flat_data[g_indexes[m]])))\n",
    "        m_target.append(target[g_indexes[n]]+' '+target[g_indexes[m]])\n",
    "        matches.append((g_indexes[n],g_indexes[m]))\n",
    "\n",
    "    # matches=[]\n",
    "    # for i in range(0,xn): # Creates xn matchings: Genuine - Forgery\n",
    "    #     n=randint(0,len(g_indexes)-1)\n",
    "    #     m=randint(0,len(f_indexes)-1)\n",
    "    #     while (g_indexes[n],f_indexes[m]) in matches:\n",
    "    #         n=randint(0,len(g_indexes)-1)\n",
    "    #         m=randint(0,len(f_indexes)-1)\n",
    "    #     m_flat_data.append(np.concatenate((flat_data[g_indexes[n]],flat_data[f_indexes[m]])))\n",
    "    #     m_target.append(target[g_indexes[n]]+' '+target[f_indexes[m]])\n",
    "    #     matches.append((g_indexes[n],f_indexes[m]))\n",
    "\n",
    "\n",
    "    matches=[]\n",
    "    for i in range(0,xn): # Creates xn matchings: Forgery - Forgery\n",
    "        n=randint(0,len(f_indexes)-1)\n",
    "        m=randint(0,len(f_indexes)-1)\n",
    "        while (f_indexes[n],f_indexes[m]) in matches:\n",
    "            n=randint(0,len(f_indexes)-1)\n",
    "            m=randint(0,len(f_indexes)-1)\n",
    "        m_flat_data.append(np.concatenate((flat_data[f_indexes[n]],flat_data[f_indexes[m]])))\n",
    "        m_target.append(target[f_indexes[n]]+' '+target[f_indexes[m]])\n",
    "        matches.append((f_indexes[n],f_indexes[m]))\n",
    "\n",
    "    return Bunch(data=m_flat_data,\n",
    "                 target=m_target\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_4(flat_data, target, xn):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "\n",
    "    - flat_data: a list of ndarray containing the flattern information coming from\n",
    "      the hog on the signature image.\n",
    "    - target: a list of strings. Each component can be 'Genuine' of 'Forgery'.\n",
    "    - xn: numbers of matches to be created for every sub_directory\n",
    "\n",
    "    Returns\n",
    "\n",
    "    A dataset divided in:\n",
    "    - data: a list of ndarray, each one originating from the concatenation of 4 array of the input flat_data.\n",
    "    - target: a list of strings, each one deriving from the concatenation of 4 strings of input target list.\n",
    "\n",
    "    -------------------------------------------------------\n",
    "\n",
    "    Rule of matching:\n",
    "    - The first xn couples are composed of 4 genuine signatures randomly choosen between the 10 available.\n",
    "    - The latter xn couples are composed of 3 genuine signature and 1 forgery signatures randomly choosen.\n",
    "    \"\"\"\n",
    "\n",
    "    m_flat_data=[]\n",
    "    m_target=[]\n",
    "    g_indexes=[ind for ind, t in enumerate(target) if t=='Genuine']\n",
    "    f_indexes=[ind for ind, t in enumerate(target) if t=='Forgery']\n",
    "\n",
    "    matches=[]\n",
    "    for i in range(0,xn): # Creates xn matchings: GGGG\n",
    "        n=randint(0,len(g_indexes)-1)\n",
    "        m1=randint(0,len(g_indexes)-1)\n",
    "        m2=randint(0,len(g_indexes)-1)\n",
    "        m3=randint(0,len(g_indexes)-1)\n",
    "        while ((n == m1) or (n == m2) or (n == m3) or\n",
    "                (m1 == m2) or (m1 == m3) or\n",
    "                (m2 == m3) or\n",
    "                (g_indexes[m1], g_indexes[m2], g_indexes[m3], g_indexes[n]) in matches):\n",
    "            n=randint(0,len(g_indexes)-1)\n",
    "            m1=randint(0,len(g_indexes)-1)\n",
    "            m2=randint(0,len(g_indexes)-1)\n",
    "            m3=randint(0,len(g_indexes)-1)\n",
    "        m_flat_data.append(np.concatenate((flat_data[g_indexes[m1]],flat_data[g_indexes[m2]],flat_data[g_indexes[m3]],flat_data[g_indexes[n]])))\n",
    "        m_target.append(target[g_indexes[m1]]+' '+target[g_indexes[m2]]+' '+target[g_indexes[m3]]+' '+target[g_indexes[n]])\n",
    "        matches.append((g_indexes[m1],g_indexes[m2],g_indexes[m3],g_indexes[n]))\n",
    "\n",
    "    matches=[]\n",
    "    for i in range(0,xn): # Creates xn matchings: FFFF\n",
    "        n=randint(0,len(f_indexes)-1)\n",
    "        m1=randint(0,len(f_indexes)-1)\n",
    "        m2=randint(0,len(f_indexes)-1)\n",
    "        m3=randint(0,len(f_indexes)-1)\n",
    "        while ((n == m1) or (n == m2) or (n == m3) or\n",
    "                (m1 == m2) or (m1 == m3) or\n",
    "                (m2 == m3) or\n",
    "                (f_indexes[m1], f_indexes[m2], f_indexes[m3], f_indexes[n]) in matches):\n",
    "            n=randint(0,len(f_indexes)-1)\n",
    "            m1=randint(0,len(f_indexes)-1)\n",
    "            m2=randint(0,len(f_indexes)-1)\n",
    "            m3=randint(0,len(f_indexes)-1)\n",
    "        m_flat_data.append(np.concatenate((flat_data[f_indexes[m1]],flat_data[f_indexes[m2]],flat_data[f_indexes[m3]],flat_data[f_indexes[n]])))\n",
    "        m_target.append(target[f_indexes[m1]]+' '+target[f_indexes[m2]]+' '+target[f_indexes[m3]]+' '+target[f_indexes[n]])\n",
    "        matches.append((f_indexes[m1],f_indexes[m2],f_indexes[m3],f_indexes[n]))\n",
    "\n",
    "\n",
    "    # matches=[]\n",
    "    # for i in range(0,xn): # Creates xn matchings: Genuine - Forgery\n",
    "    #     n=randint(0,len(g_indexes)-1)\n",
    "    #     m=randint(0,len(f_indexes)-1)\n",
    "    #     while (g_indexes[n],f_indexes[m]) in matches:\n",
    "    #         n=randint(0,len(g_indexes)-1)\n",
    "    #         m=randint(0,len(f_indexes)-1)\n",
    "    #     m_flat_data.append(np.concatenate((flat_data[g_indexes[n]],flat_data[f_indexes[m]])))\n",
    "    #     m_target.append(target[g_indexes[n]]+' '+target[f_indexes[m]])\n",
    "    #     matches.append((g_indexes[n],f_indexes[m]))\n",
    "\n",
    "\n",
    "    # matches=[]\n",
    "    # for i in range(0,xn): # Creates xn matchings: GGGF\n",
    "    #     n=randint(0,len(g_indexes)-1)\n",
    "    #     m1=randint(0,len(g_indexes)-1)\n",
    "    #     m2=randint(0,len(g_indexes)-1)\n",
    "    #     m3=randint(0,len(g_indexes)-1)\n",
    "    #     while ((m1 == m2) or (m1 == m3) or (m2 == m3) or\n",
    "    #            (g_indexes[m1], g_indexes[m2], g_indexes[m3], f_indexes[n]) in matches):\n",
    "    #            n=randint(0,len(g_indexes)-1)\n",
    "    #            m1=randint(0,len(g_indexes)-1)\n",
    "    #            m2=randint(0,len(g_indexes)-1)\n",
    "    #            m3=randint(0,len(g_indexes)-1)\n",
    "    #     m_flat_data.append(np.concatenate((flat_data[g_indexes[m1]],flat_data[g_indexes[m2]],flat_data[g_indexes[m3]],flat_data[f_indexes[n]])))\n",
    "    #     m_target.append(target[g_indexes[m1]]+' '+target[g_indexes[m2]]+' '+target[g_indexes[m3]]+' '+target[f_indexes[n]])\n",
    "    #     matches.append((g_indexes[m1],g_indexes[m2],g_indexes[m3],f_indexes[n]))\n",
    "\n",
    "\n",
    "    return Bunch(data=m_flat_data,\n",
    "                 target=m_target\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_files(container_path, dimension=(100, 144)): # height x lenght\n",
    "    \"\"\"\n",
    "    Load image files with categories as subfolder names\n",
    "    which performs like scikit-learn sample dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    container_path : string or unicode\n",
    "        Path to the main folder holding one subfolder per category\n",
    "    dimension : tuple\n",
    "        size to which image are adjusted to\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Bunch\n",
    "    \"\"\"\n",
    "    image_dir = Path(container_path)\n",
    "    subj = [directory for directory in image_dir.iterdir() if directory.is_dir()]\n",
    "    categories = ['Genuine', 'Forgery']\n",
    "\n",
    "    descr = \"An image classification dataset\"\n",
    "    #images = []\n",
    "    flat_data = []\n",
    "    target = []\n",
    "\n",
    "    for n, s in enumerate(subj):\n",
    "        folders = [sub_directory for sub_directory in s.iterdir() if sub_directory.is_dir()]\n",
    "        temp_flat_data=[] # Temporary list of flat_data used to store the information of the current subject\n",
    "                          # and reinitialized as empty list for each subject\n",
    "        temp_images=[] # Temporary list of images\n",
    "        temp_target=[] # Temporary list of target\n",
    "        for sub_dir in folders:\n",
    "            if sub_dir.name in categories: # Skip the Disguised folder\n",
    "                for file in sub_dir.iterdir():\n",
    "                    img_gray = img_as_ubyte(io.imread(file, as_gray=True)) # load the images in grayscale\n",
    "                    img_bin = binarize(img_gray)\n",
    "                    img_resized = resize(img_bin, dimension, anti_aliasing=True, mode='reflect')\n",
    "\n",
    "\n",
    "                    temp_flat_data.append(HOG(img_resized))\n",
    "\n",
    "                    # NOTE: resize() produces a dtype('float64') while imread() and binarize() return dtype('uint8')\n",
    "                    # the opening filter is bad\n",
    "\n",
    "                    temp_target.append(sub_dir.name)\n",
    "\n",
    "        #subj_dataset = match_2(temp_flat_data, temp_target, 5) #now the number of couples can be setted\n",
    "\n",
    "        subj_dataset = match_2(temp_flat_data, temp_target, 5) #now the number of 4tuples can be setted\n",
    "        flat_data += subj_dataset.data\n",
    "        target += subj_dataset.target\n",
    "\n",
    "    flat_data = np.array(flat_data)\n",
    "    target = np.array(target)\n",
    "\n",
    "    return Bunch(data=flat_data,\n",
    "                 target=target,\n",
    "                 target_names=categories,\n",
    "                 DESCR=descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To let the code run it is necessary to put this script in the same folder which contains the database folder\n",
    "\n",
    "db_folder='SignUniPD_anonymised' # Modify with the name of the folder containing the database.\n",
    "image_dataset = load_image_files(os.path.join(os.getcwd(),db_folder))\n",
    "\n",
    "print(\"----Dataset Loaded----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data in Training Set and Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_dataset.data, image_dataset.target, test_size=0.3,random_state=109)\n",
    "\n",
    "print(\"----Dataset Splitted----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Set shape: {}\".format(X_train.shape))\n",
    "print(\"Test Set shape: {}\".format(X_test.shape))\n",
    "\n",
    "print(\"Training Set shape: {}\".format(y_train.shape))\n",
    "print(\"Test Set shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data with parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davide/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']}, {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"----Training----\")\n",
    "\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, param_grid, cv=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"----Training Ended----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for - \n",
      "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
      "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False),\n",
      "       fit_params=None, iid='warn', n_jobs=None,\n",
      "       param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']}, {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring=None, verbose=0):\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Genuine Forgery       0.65      0.75      0.70       235\n",
      "Genuine Genuine       0.71      0.61      0.66       239\n",
      "\n",
      "      micro avg       0.68      0.68      0.68       474\n",
      "      macro avg       0.68      0.68      0.68       474\n",
      "   weighted avg       0.68      0.68      0.68       474\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report for - \\n{}:\\n{}\\n\".format(\n",
    "    clf, metrics.classification_report(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  2019-01-03 21:09:53.882319\n",
      "End time:  2019-01-03 21:09:53.884219\n",
      "Uptime : 3:47:01.685897\n"
     ]
    }
   ],
   "source": [
    "# End point\n",
    "end_time = time.time()\n",
    "\n",
    "uptime = end_time - start_time\n",
    "\n",
    "human_uptime = datetime.timedelta(seconds=uptime)\n",
    "\n",
    "print(\"End time: \", datetime.datetime.now())\n",
    "print(\"Uptime :\" ,human_uptime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
